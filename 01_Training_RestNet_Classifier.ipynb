{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a ResNet classifier to classify mushrooms\n",
    "In this notebook, we will train and save a working ResNet model based on the ResNet9 architechture (LINK) to classify images of mushrooms in the norwegian flora. To this end, we first need to create a pipeline to load in our data, preprocess it and feed it to a training loop in mini-batches. We must further design the residual convolutional blocks used in the ResNet, as well as the final model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset and preprocessing of data\n",
    "Before declaring residual convolutional blocks and the ResNet model, we should make sure all data can be loaded, preprocessed and iterated over in a consistent, precise manner. To this end, we will declare a PyTorch dataset and a PyTorch preprocessing step, all present and pre-loaded into a dataset instance `data`. \n",
    "### Defining a PyTorch dataset for image data\n",
    "The image data and subsequent labels will be accessed and loaded into memory using a custom `MushroomDataset`-class, inheriting from PyTorch standard dataset-class in `torch.utils.Dataset`. Not only will creating a separate class streamline the retrieval and preprocessing of data, the inherited functionality allows for the seamless division into mini-batches using PyTorch Dataloaders, which should allow for better, less resource intensive training down the line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a custom dataset to simplify the use and indexing of the custom mushroom dataset\n",
    "class MushroomDataset(Dataset):\n",
    "    # Overload the init funtion to capture the image directory, transform and load in the labels\n",
    "    def __init__(self, path_imgs: str, path_labels: str, transform = None) -> None:\n",
    "        self.img_dir = path_imgs\n",
    "        self.labels = pd.read_csv(path_labels)\n",
    "        self.transform = transform\n",
    "    \n",
    "    # Overload the len(..) operator to give the length of all labels\n",
    "    def __len__(self) -> int:\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    # Overload the [index] indexator to yield an image (s.t transforms) and it's corresponding label\n",
    "    def __getitem__(self, index):\n",
    "        # Find the image path and load the image\n",
    "        img_path = Path(f\"{self.img_dir}/{self.labels.iloc[index, 0]}.jpg\")\n",
    "        img = plt.imread(img_path)\n",
    "\n",
    "        # Load the corresponding image label\n",
    "        label = torch.tensor(self.labels.iloc[index, 1], dtype=torch.int16)\n",
    "\n",
    "        # If a transform is specified, apply it to the image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a preprocessing pipeline\n",
    "The `MushroomDataset`-class contains a `transform` parameter, which will be used to apply a set of simple, yet important, preprocessing steps to the image data. Essentially, we wish to normalize all color channels of the image data for the better convergence of the employed nonlinear optimization scheme during training, as well as transform the data into PyTorch tensors.\n",
    "\n",
    "Before defining the preprocessing pipeline however, we need to note the average mean- and standard deviation of all color channels across our dataset. This will serve as the backbone for our normalization scheme, and the values should be found experimentally. Below, the mean and standard deviations of the separate color channels of all images are accumulated into `mean_liet` and `std_list`, before being averaged and returned. This yields the necessary data for our normalization pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"01_Training_RestNet_Classifier.ipynb\").parent.resolve()\n",
    "\n",
    "# Find a list of all .jpg image-files in the dataset\n",
    "img_paths = Path(f\"{BASE_DIR}/data/mushroom_imgs\").rglob('*.jpg')\n",
    "\n",
    "# Find the mean of all color channels by accumulating each value over all available images\n",
    "mean_list, std_list = np.array([0, 0, 0]), np.array([0, 0, 0])\n",
    "\n",
    "for count, path in enumerate(img_paths):\n",
    "    # Load in the image, convert it to a torch.Tensor and permute for correct dimensions\n",
    "    img = torch.Tensor(plt.imread(str(path))).permute((2, 0, 1))\n",
    "    \n",
    "    # Perform elementwise addition using np.add\n",
    "    mean_list = np.add(mean_list, img.mean([1, 2]))\n",
    "    std_list = np.add(std_list, img.std([1, 2]))\n",
    "\n",
    "# Perform elementwise division with the counter to get the average mean/std of all color channels across all images \n",
    "mean_list, std_list = mean_list / (count+1), std_list / (count + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the mean and standard deviations, we can define a simple preprocessing pipeline using a composite transformation from `torchvision.transforms.Compose`. An image fed to the composite transformation will first be converted into a PyTorch tensor, before being normalized accross all available color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define a composite transform to preprocess the data\n",
    "preprocessing_pipeline = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(list(mean_list), list(std_list))\n",
    "])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training/test sets w. Dataloaders\n",
    "An instance `data` of the `MushroomDataset`-class, preprocessed using `preprocessing_pipeline`, can be split into a training and test set using `torch.utils.data.random_split()`. Here, this will be done using 25% of the data for validation:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the absolute paths to the image data and subsequent labels\n",
    "IMAGE_DIR = Path(f\"{BASE_DIR}/data/mushroom_imgs\")\n",
    "LABEL_DIR = Path(f\"{BASE_DIR}/data/mushroom_imgs/img_labels.csv\")\n",
    "\n",
    "# Instantiate the dataset\n",
    "data = MushroomDataset(IMAGE_DIR, LABEL_DIR, preprocessing_pipeline)\n",
    "\n",
    "# Divide the dataset into training and test sets using pytorch's 'random_split' method:\n",
    "train_data, test_data = torch.utils.data.random_split(data, [0.75, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each subset of data, we can now create a dataloader from `torch.utils.data.Dataloader`, allowing us to iterate through the dataset in shuffled mini-batches: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the BATCH_SIZE hyperparameter deciding the amount of images in each mini-batch during training\n",
    "# NOTE: This should be tuned as a hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Declare the dataloaders\n",
    "train_dataloader = DataLoader(dataset = train_data,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              shuffle = True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset = test_data,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the ResNet Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
