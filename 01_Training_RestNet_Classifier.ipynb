{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a ResNet classifier to classify mushrooms\n",
    "In this notebook, we will train and save a working ResNet model based on the ResNet50 architecture to classify images of mushrooms in the norwegian flora. To this end, we first need to create a pipeline to load in our data, preprocess it and feed it to a training loop in mini-batches. We must further design the residual convolutional blocks used in the ResNet, as well as the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset and preprocessing of data\n",
    "Before declaring residual convolutional blocks and the ResNet model, we should make sure all data can be loaded, preprocessed and iterated over in a consistent, precise manner. To this end, we will declare a PyTorch dataset and a PyTorch preprocessing step, all present and pre-loaded into a dataset instance `data`. \n",
    "### Defining a PyTorch dataset for image data\n",
    "The image data and subsequent labels will be accessed and loaded into memory using a custom `MushroomDataset`-class, inheriting from PyTorch standard dataset-class in `torch.utils.Dataset`. Not only will creating a separate class streamline the retrieval and preprocessing of data, the inherited functionality allows for the seamless division into mini-batches using PyTorch Dataloaders, which should allow for better, less resource intensive training down the line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a custom dataset to simplify the use and indexing of the custom mushroom dataset\n",
    "class MushroomDataset(Dataset):\n",
    "    # Overload the init function to capture the image directory, transform and load in the labels\n",
    "    def __init__(self, path_imgs: str, path_labels: str, transform = None) -> None:\n",
    "        self.img_dir = path_imgs\n",
    "        self.labels = pd.read_csv(path_labels)\n",
    "        self.transform = transform\n",
    "    \n",
    "    # Overload the len(..) operator to give the length of all labels\n",
    "    def __len__(self) -> int:\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    # Overload the [index] indexator to yield an image (s.t transforms) and it's corresponding label\n",
    "    def __getitem__(self, index):\n",
    "        # Find the image path and load the image\n",
    "        img_path = Path(f\"{self.img_dir}/{self.labels.iloc[index, 0]}.jpg\")\n",
    "        img = plt.imread(img_path)\n",
    "\n",
    "        # Load the corresponding image label\n",
    "        label = torch.tensor(self.labels.iloc[index, 1])\n",
    "\n",
    "        # If a transform is specified, apply it to the image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, label)\n",
    "    \n",
    "    # Define a quick function to fetch the amount of available classes present in the dataset\n",
    "    def _num_classes(self) -> int:\n",
    "        return len(self.labels.value_counts('label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a preprocessing pipeline\n",
    "The `MushroomDataset`-class contains a `transform` parameter, which will be used to apply a set of simple, yet important, preprocessing steps to the image data. Essentially, we wish to normalize all color channels of the image data for the better convergence of the employed nonlinear optimization scheme during training, as well as transform the data into PyTorch tensors.\n",
    "\n",
    "Before defining the preprocessing pipeline however, we need to note the average mean- and standard deviation of all color channels across our dataset. This will serve as the backbone for our normalization scheme, and the values should be found experimentally. Below, the mean and standard deviations of the separate color channels of all images are accumulated into `mean_liet` and `std_list`, before being averaged and returned. This yields the necessary data for our normalization pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"01_Training_RestNet_Classifier.ipynb\").parent.resolve()\n",
    "\n",
    "# Find a list of all .jpg image-files in the dataset\n",
    "img_paths = Path(f\"{BASE_DIR}/data/mushroom_imgs\").rglob('*.jpg')\n",
    "\n",
    "# Find the mean of all color channels by accumulating each value over all available images\n",
    "mean_list, std_list = np.array([0, 0, 0]), np.array([0, 0, 0])\n",
    "\n",
    "for count, path in enumerate(img_paths):\n",
    "    # Load in the image, convert it to a torch.Tensor and permute for correct dimensions\n",
    "    img = torch.Tensor(plt.imread(str(path))).permute((2, 0, 1))\n",
    "    \n",
    "    # Perform elementwise addition using np.add\n",
    "    mean_list = np.add(mean_list, img.mean([1, 2]))\n",
    "    std_list = np.add(std_list, img.std([1, 2]))\n",
    "\n",
    "# Perform elementwise division with the counter to get the average mean/std of all color channels across all images \n",
    "mean_list, std_list = mean_list / (count+1), std_list / (count + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the mean and standard deviations, we can define a simple preprocessing pipeline using a composite transformation from `torchvision.transforms.Compose`. An image fed to the composite transformation will first be converted into a PyTorch tensor, before being normalized accross all available color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define a composite transform to preprocess the data\n",
    "preprocessing_pipeline = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(list(mean_list), list(std_list))\n",
    "])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training/test sets w. Dataloaders\n",
    "An instance `data` of the `MushroomDataset`-class, preprocessed using `preprocessing_pipeline`, can be split into a training and test set using `torch.utils.data.random_split()`. Here, this will be done using 25% of the data for validation:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the absolute paths to the image data and subsequent labels\n",
    "IMAGE_DIR = Path(f\"{BASE_DIR}/data/mushroom_imgs\")\n",
    "LABEL_DIR = Path(f\"{BASE_DIR}/data/mushroom_imgs/img_labels.csv\")\n",
    "\n",
    "# Instantiate the dataset\n",
    "data = MushroomDataset(IMAGE_DIR, LABEL_DIR, preprocessing_pipeline)\n",
    "\n",
    "# Divide the dataset into training and test sets using pytorch's 'random_split' method:\n",
    "train_data, test_data = torch.utils.data.random_split(data, [0.75, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each subset of data, we can now create a dataloader from `torch.utils.data.Dataloader`, allowing us to iterate through the dataset in shuffled mini-batches: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the BATCH_SIZE hyperparameter deciding the amount of images in each mini-batch during training\n",
    "# NOTE: This should be tuned as a hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define the amounts of workers on each dataloader\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Declare the dataloaders\n",
    "train_dataloader = DataLoader(dataset = train_data,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              shuffle = True,\n",
    "                              num_workers = NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(dataset = test_data,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            shuffle = True,\n",
    "                            num_workers = NUM_WORKERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the ResNet Model\n",
    "In the following subsection, I will attempt to walk us through the building of a ResNet model from scratch, mainly for the purposes of attaining deep knowledge about the topic and methods used in the implementation. The target output, a general `ResNet`-class, implements what are known as \"*deeper ResNet architectures*\". These encompass ResNet50, 101, 152 and above; utilizing *bottlenecks*, defined in the [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf), to compress the data going into a block. This implies that the implementation is not ideal for shallower ResNet architectures, like ResNet9, 18 or 34.\n",
    "\n",
    "The proposed architecture cosists of 5 **layers**, each built up by **residual blocks**. To this end, out implementation will seek to build a class `ResNetLayer` encompassing another class `ResNetBlock`. `ResNetLayer` instances are later concated in the final `ResNet` class, which streamlines the flow of data through said layers.  \n",
    "\n",
    "The following section will try to streamline the somewhat tricky implementation in an orderly fashion. However, due to the complexity of dimensional operations, it is highly recommended to consult the [original paper](https://arxiv.org/pdf/1512.03385.pdf), especially Table 1, which presents the main outlines.\n",
    "\n",
    "### The ResNet block\n",
    "A ResNet block generally seeks to do two things: \n",
    "1. Extract learnt features of the input data `x`\n",
    "2. Combine the input data and said features at the output, effectively augmenting, rather than losing, information. \n",
    "The initial features are attained using standard, trained convolutional layers, whilst the final combination is, more often than not, a simple element-wise addition. The theoretical workings of a block are, in this sense, fairly straightforward. \n",
    "\n",
    "The first part of the procedure is, however, somewhat complicated by the addition of the aforementioned *bottlenecks*. These change the otherwise simple block-structure to a meticulously crafted stack of convolutional layers, which are used to reduce the amount of parameters necessary to train deeper networks. The first layer, `conv_red` in the below class implementation, seek to reduce the input data to a lower dimensionality by a 1x1 convolution, often by a fixed factor of `4`. This reduced input data `x` is then fed to the second layer `conv_features`, which infers on said data with a 3x3 convolution. Finally, the dimensionality of the extracted features are increased another 1x1 convolution `conv_inc`. Overall, such a bottleneck reduces the amounts of parameters per block significantly, which yields a massive improvement to the training-performance of deeper networks.\n",
    "\n",
    "*Bottlenecks*, however, also impact the second part, as the reduction/increase of dimensionalities have to be accounted for in the final augmentation-step. When two PyTorch tensors are added together, their dimensions must match perfectly, to which another 1x1 convolution is adopted. The convolution `identity_downsample`, is used to alter the dimensionality of the input `x` if it doesn't match the dimensions of the extracted features. `identity_downsample` is taken in as a parameter to the block and is defined as part of the `ResNetLayer`-class, described in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Define a ResNet block\n",
    "# NOTE: Following the structure in the article, note that identity mappings increase/decrease dimensions by a factor 4 and that paddings are fixed. \n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels:int,\n",
    "                 identity_mapping: nn.Sequential = None,\n",
    "                 stride: int = 1):\n",
    "        super().__init__()\n",
    "        # The factor of which to increase/reduce dimensions\n",
    "        self.dim_ext_factor = 4\n",
    "        \n",
    "        # The first convolutional layer to reduce dimensionality before a bottleneck, as well as a connected BN computation\n",
    "        self.conv_red = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channels,\n",
    "                      out_channels = out_channels,\n",
    "                      kernel_size = 1,\n",
    "                      stride = 1,\n",
    "                      padding = 0,\n",
    "                      bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # The second convolutional layer to infer on the input data. Again followed by a BN computation. NOTE: Only layer with variable stride\n",
    "        self.conv_features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = out_channels,\n",
    "                      out_channels = out_channels,\n",
    "                      kernel_size = 3,\n",
    "                      stride = stride,\n",
    "                      padding = 1,\n",
    "                      bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # The third convolutional layer, in which the dimensions are increased by a factor 'dim_ext_factor'.\n",
    "        self.conv_inc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = out_channels,\n",
    "                      out_channels = out_channels * self.dim_ext_factor,\n",
    "                      kernel_size = 1,\n",
    "                      stride = 1,\n",
    "                      padding = 0,\n",
    "                      bias = False),\n",
    "            nn.BatchNorm2d(out_channels * self.dim_ext_factor),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Define a ReLU block used to infer on the output\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Finally, allocate an instance-variable to the desired identity mapping:\n",
    "        self.identity_mapping = identity_mapping\n",
    "\n",
    "    # Define the forward function -> How data 'x' is processed by the block\n",
    "    def forward(self, x):\n",
    "        # save the original input for later addition\n",
    "        block_input = x\n",
    "\n",
    "        # propagate data through the layers using Conv2D -> BN -> ReLu\n",
    "        x = self.conv_red(x)\n",
    "        x = self.conv_features(x)\n",
    "        x = self.conv_inc(x)\n",
    "\n",
    "        # if an identity-mapping is supplied and the input dimensions don't match the data, use the mapping to downsample 'block input'\n",
    "        if self.identity_mapping and block_input.shape != x.shape:\n",
    "            block_input = self.identity_mapping(block_input)\n",
    "        \n",
    "        # add the input to the residual information -> Information augmentation\n",
    "        # NOTE: Gradient issue right here\n",
    "        x = x + block_input\n",
    "\n",
    "        return self.relu(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ResNet Layer\n",
    "A ResNet Layer seeks to propagate the input data `x` through `num_blocks` `ResNetBlock` instances using the same amount of filters/convolutional kernels. A complete residual network consists of multiple such layers with different amounts of blocks working on different dimensionalities of data. The entire purpose of the below class is then to implement a layer, as described by Table 1 in the [ResNet article](https://arxiv.org/pdf/1512.03385.pdf).\n",
    "\n",
    "In said article, one can see that the dimensionality within each layer is increased by a `dim_ext_factor` of 4. This is hardcoded as an instance-parameter, and is used throughout the layer. \n",
    "\n",
    "Firstly, the layer-implementation is tasked with the declaration of an `identity mapping` if the amount of convolutional kernels in the input `in_channels` is not `dim_ext_factor` proportional to the desired amount of kernels in the output `out_channels`. A simple sequential operation is then declared, with a 1x1 convolution correctly scaling the input data as well as a simple 2D batch-norm operation. \n",
    "\n",
    "Secondly, the layer is tasked with organizing `num_blobks` instances of `ResNetBlock`, so as to propagate the data correctly. This is simply done by adding all instances to a list, which are readily available in the later *forward* pass. The most important detail to note here is how the `in_channels` are scaled after the first block, so as to accomodate later blocks to the dimensionality increase present throughout a ResNet layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a ResNet Layer\n",
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_blocks: int,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 stride: int):\n",
    "        super().__init__()\n",
    "        # The factor of which to increase/reduce dimensions\n",
    "        self.dim_ext_factor = 4\n",
    "        \n",
    "        # Define the identity_mapping and whether or not it is needed in the current layer\n",
    "        identity_mapping = None\n",
    "        if stride != 1 or in_channels != out_channels*self.dim_ext_factor:\n",
    "            identity_mapping = nn.Sequential(\n",
    "                nn.Conv2d(in_channels = in_channels,\n",
    "                          out_channels = out_channels * self.dim_ext_factor,\n",
    "                          kernel_size = 1,\n",
    "                          stride = stride,\n",
    "                          bias = False),\n",
    "                nn.BatchNorm2d(out_channels * self.dim_ext_factor)\n",
    "            )\n",
    "        \n",
    "        # Define the layer as an instance parameter and start adding blocks to it\n",
    "        self.layer = []\n",
    "        self.layer.append(ResNetBlock(in_channels = in_channels,\n",
    "                                      out_channels = out_channels,\n",
    "                                      identity_mapping = identity_mapping,\n",
    "                                      stride = stride))\n",
    "        \n",
    "        # At the end of a ResNetBlock, the number of channels are increased by a factor of self.dim_ext_factor. The 'in_channels' of subsequent blocks have to be adjusted accordingly.\n",
    "        in_channels = out_channels * self.dim_ext_factor\n",
    "\n",
    "        # Go through the remaining blocks, appending them to the layer\n",
    "        for i in range(num_blocks - 1):\n",
    "            block = ResNetBlock(in_channels = in_channels, \n",
    "                                out_channels = out_channels,\n",
    "                                identity_mapping = identity_mapping,\n",
    "                                stride = 1)\n",
    "            self.layer.append(block)\n",
    "        \n",
    "        # Finally, make self.layer into a sequential operation\n",
    "        self.layer = nn.Sequential(*self.layer)\n",
    "\n",
    "    # Define the forward pass through the Layer -> How data 'x' is processed\n",
    "    def forward(self, x):\n",
    "        # Propagate the data through all blocks within the layer\n",
    "        #for block in self.layer:\n",
    "        #    x = block(x)\n",
    "\n",
    "        return self.layer(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep ResNet Architecture\n",
    "With `ResNetLayer` instances, we can now implement a general class for deeper ResNet architectures using *bottlenecked* residual blocks. Again following Table 1 in the [ResNet article](https://arxiv.org/pdf/1512.03385.pdf), we seek to implement 1 standard, convolutional layer, followed by 4 `ResNetLayer`, which lead to a Fully Connected Neural Network (FCNN) mapping the input to `num_classes` possible outcomes.\n",
    "\n",
    "The `ResNet` code is fairly self explanatory. The only thing one should note is how the choice of `layer_list` affects the output architecture. `layer_list` defines the amount of `ResNetBlock`-instances to be declared at each layer and is set to a ResNet50 architecture by default. By choosing `layer_list = [3, 4, 23, 3]` or `layer_list = [3, 8, 36, 3]`, the resulting architecture can be altered to ResNet101 or ResNet152 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Define the finished ResNet implementation\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 layer_list: List[int] = [3, 4, 6, 3],\n",
    "                 num_classes: int = 1,\n",
    "                 img_channels: int = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the first part of the ResNet architecture, which is shared by all sizes.\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = img_channels,\n",
    "                      out_channels = 64,\n",
    "                      kernel_size = 7,\n",
    "                      stride = 2,\n",
    "                      padding = 3,\n",
    "                      bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3,\n",
    "                         stride = 2,\n",
    "                         padding = 1)\n",
    "        )\n",
    "\n",
    "        # Define the 4 ResNetLayer's \n",
    "        self.layer_2 = ResNetLayer(layer_list[0], in_channels= 64, out_channels= 64, stride = 1)\n",
    "        self.layer_3 = ResNetLayer(layer_list[1], in_channels= 256, out_channels= 128, stride = 2)\n",
    "        self.layer_4 = ResNetLayer(layer_list[2], in_channels= 512, out_channels= 256, stride = 2)\n",
    "        self.layer_5 = ResNetLayer(layer_list[3], in_channels= 1024, out_channels= 512, stride = 2)\n",
    "        \n",
    "        # Construct the final FCNN, preempted by an average pooling operation and a flattening of the 3D image data:\n",
    "        self.fcnn = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = 512 * 4,\n",
    "                      out_features = num_classes)\n",
    "        )\n",
    "\n",
    "    # Define the final forward pass through the ResNet -> How data 'x' is processed by the block\n",
    "    def forward(self, x):\n",
    "        # Pass the data through all layers:\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = self.layer_5(x)\n",
    "\n",
    "        # Perform average pooling on the data, flatten it and feed it through the final fcnn\n",
    "        return self.fcnn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some hyperparameters for the training/test-loops\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Resnet model\n",
    "resNet50 = ResNet(layer_list = [3, 4, 6, 3], \n",
    "                  num_classes = data._num_classes(), \n",
    "                  img_channels = 3)\n",
    "\n",
    "# Define a loss funciton\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(params = resNet50.parameters(),\n",
    "                             lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10\n",
      "-------\n",
      "Considered 0/1830\n",
      "Considered 352/1830\n",
      "Considered 704/1830\n",
      "Considered 1056/1830\n",
      "Considered 1408/1830\n",
      "Considered 1760/1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [04:44<42:40, 284.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 7.3172 | Test Loss: 3.9440 | Test Accuracy: 0.0594\n",
      "Epoch: 1/10\n",
      "-------\n",
      "Considered 0/1830\n",
      "Considered 352/1830\n",
      "Considered 704/1830\n",
      "Considered 1056/1830\n",
      "Considered 1408/1830\n",
      "Considered 1760/1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [09:03<35:56, 269.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 3.4805 | Test Loss: 3.5276 | Test Accuracy: 0.0469\n",
      "Epoch: 2/10\n",
      "-------\n",
      "Considered 0/1830\n",
      "Considered 352/1830\n",
      "Considered 704/1830\n",
      "Considered 1056/1830\n",
      "Considered 1408/1830\n",
      "Considered 1760/1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [13:18<30:39, 262.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 3.4814 | Test Loss: 3.4633 | Test Accuracy: 0.0484\n",
      "Epoch: 3/10\n",
      "-------\n",
      "Considered 0/1830\n",
      "Considered 352/1830\n",
      "Considered 704/1830\n",
      "Considered 1056/1830\n",
      "Considered 1408/1830\n",
      "Considered 1760/1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [17:33<25:59, 259.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 3.4722 | Test Loss: 3.4587 | Test Accuracy: 0.0484\n",
      "Epoch: 4/10\n",
      "-------\n",
      "Considered 0/1830\n",
      "Considered 352/1830\n",
      "Considered 704/1830\n",
      "Considered 1056/1830\n",
      "Considered 1408/1830\n",
      "Considered 1760/1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [21:48<21:30, 258.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 3.4324 | Test Loss: 3.4354 | Test Accuracy: 0.0484\n",
      "Epoch: 5/10\n",
      "-------\n",
      "Considered 0/1830\n",
      "Considered 352/1830\n",
      "Considered 704/1830\n",
      "Considered 1056/1830\n",
      "Considered 1408/1830\n",
      "Considered 1760/1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [25:55<16:56, 254.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 3.4308 | Test Loss: 3.4434 | Test Accuracy: 0.0406\n",
      "Epoch: 6/10\n",
      "-------\n",
      "Considered 0/1830\n",
      "Considered 352/1830\n",
      "Considered 704/1830\n",
      "Considered 1056/1830\n",
      "Considered 1408/1830\n",
      "Considered 1760/1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [30:01<12:34, 251.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 3.4181 | Test Loss: 3.4051 | Test Accuracy: 0.0484\n",
      "Epoch: 7/10\n",
      "-------\n",
      "Considered 0/1830\n",
      "Considered 352/1830\n",
      "Considered 704/1830\n",
      "Considered 1056/1830\n",
      "Considered 1408/1830\n",
      "Considered 1760/1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [39:09<11:31, 345.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 3.4092 | Test Loss: 3.3951 | Test Accuracy: 0.0469\n",
      "Epoch: 8/10\n",
      "-------\n",
      "Considered 0/1830\n",
      "Considered 352/1830\n",
      "Considered 704/1830\n",
      "Considered 1056/1830\n",
      "Considered 1408/1830\n",
      "Considered 1760/1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [43:29<05:19, 319.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 3.4011 | Test Loss: 3.3937 | Test Accuracy: 0.0484\n",
      "Epoch: 9/10\n",
      "-------\n",
      "Considered 0/1830\n",
      "Considered 352/1830\n",
      "Considered 704/1830\n",
      "Considered 1056/1830\n",
      "Considered 1408/1830\n",
      "Considered 1760/1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [47:54<00:00, 287.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 3.4025 | Test Loss: 3.4172 | Test Accuracy: 0.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # using tqdm to initialize a progress bar\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the training/test loops for a set amount of epochs\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(f\"Epoch: {epoch}/{EPOCHS}\\n-------\")\n",
    "\n",
    "    # Set the model into 'training' mode (tracking gradients)\n",
    "    resNet50.train()\n",
    "\n",
    "    # Define a variable to hold the accumulated training loss over all training-batches\n",
    "    train_loss = 0\n",
    "\n",
    "    # Define the training loop, performed on each batch of training-data\n",
    "    for batch, (img, label) in enumerate(train_dataloader):\n",
    "        # Perform a prediction on the images in the batch\n",
    "        y_pred = resNet50(img)\n",
    "\n",
    "        # Calculate the loss of the batch (and accumulate total training loss)\n",
    "        loss = loss_fn(y_pred, label) \n",
    "        train_loss += loss\n",
    "        \n",
    "        # Reset the gradients of the optimizer \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform backpropagation on the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust model weights/biases through an optimizer step\n",
    "        optimizer.step()\n",
    "    \n",
    "        # print out some diagnostic of the training \n",
    "        if batch%(len(train_dataloader)//5) == 0:\n",
    "            print(f\"Considered {batch*BATCH_SIZE}/{len(train_dataloader.dataset)}\")\n",
    "    \n",
    "    # divide the training loss over all batches\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Set the model to training mode (turns off gradient tracking)\n",
    "    resNet50.eval()\n",
    "\n",
    "    # # Define a variable to hold the accumulated test-loss and test accuracy over all test-batches\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Define the testing loop, performed on each batch of testing-data\n",
    "    with torch.inference_mode():\n",
    "        for img, label in test_dataloader:\n",
    "            # Perform a prediction on the images in the batch\n",
    "            test_pred = resNet50(img)\n",
    "\n",
    "            # Accumulate the test loss\n",
    "            test_loss += loss_fn(test_pred, label)\n",
    "\n",
    "            # Accumulate the test accuracy\n",
    "            test_pred = torch.argmax(test_pred, dim = 1)\n",
    "            test_acc += accuracy_score(label.numpy(), test_pred.numpy())\n",
    "\n",
    "    # Calculate the mean test loss/accuracy over all batches\n",
    "    test_loss = test_loss / len(test_dataloader)\n",
    "    test_acc = test_acc / len(test_dataloader)\n",
    "\n",
    "    # Print out some diagnostics for each epoch\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
